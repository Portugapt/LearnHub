{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47484a88-c91d-4ec5-9292-a17b7c2f7fdd",
   "metadata": {},
   "source": [
    "# Intro to Keras\n",
    "\n",
    "Fast neural network experimentations, it runs as a high level API of Tensorflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b3f95-848e-4973-a8a1-e2d87024645d",
   "metadata": {},
   "source": [
    "## Softmax predictions\n",
    "\n",
    "Your recently trained model is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the softmax activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in coords_small_test and np.argmax()to check this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bf783-b950-487f-840e-0d829bcef3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472ecaa-d89b-4f1b-8dcd-5a4960af0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc17a42-dce2-484f-9e57-c04ae578d30a",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "Essentially, use, in the output layer, a sigmoid function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee08cc-1152-459c-bac3-8e9ae7d3c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape = (20,), activation = 'relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation = 'sigmoid'))\n",
    "\n",
    "# Compile your model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss = 'binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf36f5-9417-43e5-9c30-8f9b0d032b32",
   "metadata": {},
   "source": [
    "## Comparing activation functions\n",
    "\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function get_model('relu') returns a copy of this model and applies the 'relu' activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "X_train, y_train, X_test, y_test are ready for you to use when training your models.\n",
    "\n",
    "\n",
    "    Fill up the activation functions array with relu,leaky_relu, sigmoid, and tanh.\n",
    "    Get a new model for each iteration with get_model() passing the current activation function as a parameter.\n",
    "    Fit your model providing the train and validation_data, use 20 epochs and set verbose to 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0b994-e30e-42d0-848e-a8b68ffb67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 20, verbose = 0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c865a-58c8-467d-a43e-58778a0bb97c",
   "metadata": {},
   "source": [
    "## Preparing a model for tuning\n",
    "\n",
    "Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from keras.optimizers so that you can also change its learning rate parameter.\n",
    "\n",
    "\n",
    "    Set the learning rate of the Adam optimizer object to the one passed in the arguments.\n",
    "    Set the hidden layers activations to the one passed in the arguments.\n",
    "    Pass the optimizer and the binary cross-entropy loss to the .compile() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bf0f4-9118-4ebd-bef6-9d13a001b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr = learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef0abe-b739-4588-93ad-5b7567800675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    " \n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21aae9a-df7f-4c5d-b934-cf9611800b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7351265-709e-4e45-a45c-a225b1225c63",
   "metadata": {},
   "source": [
    "# Advanced Model Architectures\n",
    "\n",
    "## Autoencoder\n",
    "\n",
    "Lets look at how backend code works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d7cd0-e17c-4905-aa99-e4bcb3b7e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499aeda-42d1-46e1-92f7-0065fde7f29b",
   "metadata": {},
   "source": [
    "The following code is interesting to do some diagnosis into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab31f16-80e1-481e-9aec-ba0755864037",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)\n",
    "    if i%4==0: \n",
    "      # Get the output of the first layer\n",
    "      layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "      # Evaluate model accuracy for this epoch\n",
    "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc35c9c-1cb5-42fb-b6e8-1c4df11804f3",
   "metadata": {},
   "source": [
    "An autoencoder with adadelta\n",
    "\n",
    "https://keras.io/api/optimizers/adadelta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546a7b7a-2499-4de8-ae86-f4fa6bec6be8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_196728/1460449261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start with a sequential model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add a dense layer with input the original image pixels and neurons the encoded representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb89fe4-39f9-481e-ae64-e1d413dd40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246851b9-11bd-46ad-81e3-67ef28562cbc",
   "metadata": {},
   "source": [
    "# CNN's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be5f11-a9b5-457e-8b23-48a08e36a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from keras.layers import Conv2D, Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6f896-a76c-4218-8111-4e0207b8e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b5e03-e59a-4e89-ae7f-48bca215cc38",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fd600-7b52-48a7-8d08-1f741e636188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1494584-fafb-4d35-8eb6-01e070b13030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a744e-f1d5-4557-80bc-1af15d5d59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b979a0-a540-46f3-9e23-42db6a214e97",
   "metadata": {},
   "source": [
    "# Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87437ce-21b0-4856-b1ce-ef573fc54d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b432cf-8900-4200-88bf-39c557d45aa5",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977de86-045f-4f22-a5ca-2aa6ad452316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Embedding, LSTM and Dense layer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = vocab_size, input_length = 3, output_dim = 8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c29996-cef9-4383-9da8-e8b42c40966a",
   "metadata": {},
   "source": [
    "## Decode your predictions\n",
    "\n",
    "Your LSTM model has already been trained (details in the previous exercise success message) so that you don't have to wait. It's time to define a function that decodes its predictions. The trained model will be passed as a default parameter to this function.\n",
    "\n",
    "Since you are predicting on a model that uses the softmax function, numpy's argmax() can be used to obtain the index/position representing the most probable next word out of the output vector of probabilities.\n",
    "\n",
    "The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model's next word prediction (which is an integer) into the actual written word it represents.\n",
    "\n",
    "You're very close to experimenting with your model!\n",
    "\n",
    "\n",
    "    Use texts_to_sequences() to turn the test_text parameter into a sequence of numbers.\n",
    "    Get the model's next word prediction by passing in test_seq . The index/position representing the word with the highest probability is obtained by calling .argmax(axis=1)[0] on the numpy array of predictions.\n",
    "    Return the word that maps to the prediction using the tokenizer's index_word dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3a159-6cde-42a6-89b8-35fe26aacaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
