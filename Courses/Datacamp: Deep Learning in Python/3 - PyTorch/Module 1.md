# Pytorch

![TorchVSNumpy](https://i.imgur.com/EuLjhYF.png)

![MM](https://i.imgur.com/U2EWr8a.png)

![ZandO](https://i.imgur.com/NoEWoRv.png)

![nn.torch](https://i.imgur.com/snk0xem.png)

We touched on Forward propagation, backward propagation, gradients in torch tensors.

![Activ](https://i.imgur.com/1mxQwEV.png)

![ReLU](https://i.imgur.com/o4GUVdH.png)

![loss](https://i.imgur.com/tVsclSA.png)

## Dataloaders

![DLTransf](https://i.imgur.com/DtojTpt.png)

![DL](https://i.imgur.com/OWB5r8d.png)

![NN Recap](https://i.imgur.com/Hm3xLed.png)

# Convolution layers

![Conv](https://i.imgur.com/Rt68jd9.png)

![Padding](https://i.imgur.com/ekvcVd5.png)

![How](https://i.imgur.com/PgSeHA7.png)

![How2](https://i.imgur.com/CHQHwYQ.png)

![how3](https://i.imgur.com/dNMTHzn.png)

# Pooling

![ooling](https://i.imgur.com/9QChXed.png)

![Max](https://i.imgur.com/ppztLG1.png)

![Maxcode](https://i.imgur.com/fN3dfyu.png)

![avg](https://i.imgur.com/xr5UFX4.png)

![avgcode](https://i.imgur.com/xAe218h.png)

# AlexNet

![COnvALex](https://i.imgur.com/FCR5KRh.png)

![AlexForward](https://i.imgur.com/Rs5pDjV.png)

# Sequential method

![seqmet](https://i.imgur.com/zh7wSDn.png)

![seqfor](https://i.imgur.com/pkv4yTb.png)

# Regularization Techniques

## L2 Regularization

## Dropout

## Batch Normalization

## Hyperparameters

## Early Stopping

# Transfer Learning

![tl](https://i.imgur.com/7JLmEJQ.png)

# End

![learned](https://i.imgur.com/wdw8WQf.png)