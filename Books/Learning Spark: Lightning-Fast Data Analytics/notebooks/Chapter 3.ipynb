{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0480ce25-f2a5-4b2e-a039-f3f291da0a3f",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2cf8f-d03e-4f9d-a5f8-5197eca9944c",
   "metadata": {},
   "source": [
    "## Spark: What’s Underneath an RDD?\n",
    "\n",
    "• Dependencies   \n",
    "• Partitions (with some locality information)   \n",
    "• Compute function: Partition => Iterator[T]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664139c-eaf6-41c0-be42-a44247dd4331",
   "metadata": {},
   "source": [
    "Spark 2.x introduced a few key schemes for structuring Spark. One is to express com‐\n",
    "putations by using common patterns found in data analysis. These patterns are\n",
    "expressed as high-level operations such as filtering, selecting, counting, aggregating,\n",
    "averaging, and grouping. This provides added clarity and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6f9ed-5ec8-46ea-ae44-5f9a8f9392c1",
   "metadata": {},
   "source": [
    "This specificity is further narrowed through the use of a set of common operators in a\n",
    "DSL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793cdc9-018e-450f-bf87-702de2dae90a",
   "metadata": {},
   "source": [
    "## Schemas and Creating DataFrames\n",
    "\n",
    "Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "    \n",
    "• You relieve Spark from the onus of inferring data types.  \n",
    "• You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.  \n",
    "• You can detect errors early if data doesn’t match the schema.   \n",
    "\n",
    "Here's an example how to define schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de523405-e41a-4cf0-9844-83ad6a2cf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/08 22:21:06 WARN Utils: Your hostname, OutOne resolves to a loopback address: 127.0.1.1; using 192.168.1.90 instead (on interface wlp8s0)\n",
      "21/08/08 22:21:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/portugapt/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/08 22:21:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "#sc.setLogLevel(newLevel)\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "        \"LinkedIn\"]],\n",
    "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "        \"LinkedIn\"]],\n",
    "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "        [\"twitter\", \"FB\"]],\n",
    "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "        [\"twitter\", \"LinkedIn\"]]\n",
    "        ]\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession\n",
    "    spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Example-3_6\")\n",
    "        .getOrCreate())\n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    # Show the DataFrame; it should reflect our table above\n",
    "    blogs_df.show()\n",
    "    # Print the schema used by Spark to process the DataFrame\n",
    "    print(blogs_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7d7cd-46d2-41d4-803b-095cfe44ef96",
   "metadata": {},
   "source": [
    "[Here's a link to the documentation on Python](http://spark.apache.org/docs/latest/api/python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ce429-c6b1-4b3e-88c3-599d7df74494",
   "metadata": {},
   "source": [
    "To start with the basics, lets look at the [columns API](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)\n",
    "\n",
    "If run into any dataType issue,\n",
    "```Column.cast(dataType)``` converts the column into type dataType.\n",
    "...\n",
    "Keep exploring.   \n",
    "\n",
    "On Rows, here are examples of how to make quick rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1549ec76-1ce1-43b2-bd04-28b79c748dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2d9e72-3067-4b7c-96a2-7abbfa74ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3efde9-a963-4e4b-b02c-d63bc8edf917",
   "metadata": {},
   "source": [
    "To read from a file: ```DataFrameReader```   \n",
    "To save to a file: ```DataFrameWriter```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a46d23-dbdd-4444-86e8-a4379d46a940",
   "metadata": {},
   "source": [
    "## Hands on a Dataset\n",
    "Quote:\n",
    "To get started, let’s read a large CSV file containing data on San Francisco Fire\n",
    "Department calls.1 As noted previously, we will define a schema for this file and use\n",
    "the DataFrameReader class and its methods to tell Spark what to do. Because this file\n",
    "contains 28 columns and over 4,380,660 records,2 it’s more efficient to define a\n",
    "schema than have Spark infer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdf764-463e-49a0-8a58-6b106ae4bfb5",
   "metadata": {},
   "source": [
    "First lets define the schema for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2e6b0a-5706-4eaa-8e2d-d77471caf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "            StructField('UnitID', StringType(), True),\n",
    "            StructField('IncidentNumber', IntegerType(), True),\n",
    "            StructField('CallType', StringType(), True),\n",
    "            StructField('CallDate', StringType(), True),\n",
    "            StructField('WatchDate', StringType(), True),\n",
    "            StructField('CallFinalDisposition', StringType(), True),\n",
    "            StructField('AvailableDtTm', StringType(), True),\n",
    "            StructField('Address', StringType(), True),\n",
    "            StructField('City', StringType(), True),\n",
    "            StructField('Zipcode', IntegerType(), True),\n",
    "            StructField('Battalion', StringType(), True),\n",
    "            StructField('StationArea', StringType(), True),\n",
    "            StructField('Box', StringType(), True),\n",
    "            StructField('OriginalPriority', StringType(), True),\n",
    "            StructField('Priority', StringType(), True),\n",
    "            StructField('FinalPriority', IntegerType(), True),\n",
    "            StructField('ALSUnit', BooleanType(), True),\n",
    "            StructField('CallTypeGroup', StringType(), True),\n",
    "            StructField('NumAlarms', IntegerType(), True),\n",
    "            StructField('UnitType', StringType(), True),\n",
    "            StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "            StructField('FirePreventionDistrict', StringType(), True),\n",
    "            StructField('SupervisorDistrict', StringType(), True),\n",
    "            StructField('Neighborhood', StringType(), True),\n",
    "            StructField('Location', StringType(), True),\n",
    "            StructField('RowID', StringType(), True),\n",
    "            StructField('Delay', FloatType(), True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcec302-65ea-4b15-9e99-271e55fce634",
   "metadata": {},
   "source": [
    "Now, lets read the dataset file. ```dfFire``` is read from the defined file, and with the schema defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d5687e-3bc3-40ec-8f3c-e6c851ea8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_fire_file = \"../repo/chapter3/data/sf-fire-calls.csv\"\n",
    "dfFire = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4baf107-ec2f-44d8-8f08-469c0e357cd7",
   "metadata": {},
   "source": [
    "To save the dataframe, for example to Parquet, which saves the schema in it's metadata, it's as simple as:   \n",
    "```python\n",
    "parquet_path = ...\n",
    "fire_df.write.format(\"parquet\").save(parquet_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0927fd-bb16-4ecf-b4a4-180eb5553018",
   "metadata": {},
   "source": [
    "Examples of some filters and projections\n",
    "A projection in relational parlance is a way to return only the\n",
    "rows matching a certain relational condition by using filters. In Spark, projections are\n",
    "done with the select() method, while filters can be expressed using the filter() or\n",
    "where() method. We can use this technique to examine specific aspects of our SF Fire\n",
    "Department data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b60dd68-3d4d-403e-bfb6-bf1226092cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfFewFires = (dfFire\n",
    "        .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "        .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "dfFewFires.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a28c05-8f2e-4c89-9638-0fd9122aeb9d",
   "metadata": {},
   "source": [
    "```countDistinct```: Returns a new Column for distinct count of col or cols.   \n",
    "What if we want to know how many distinct CallTypes were recorded as the causes\n",
    "of the fire calls? These simple and expressive queries do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b4edbe-f307-446f-adb8-0c10147f13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "(dfFire\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14838511-d3cb-439a-acc1-4222f8a9d6be",
   "metadata": {},
   "source": [
    "We can list the distinct call types in the data set using these queries:\n",
    "In Python, filter for only distinct non-null CallTypes from all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8f515a-26b4-4d54-b377-265a1dcc0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dfFire\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .distinct()\n",
    "    .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ab06f-9c5b-40a2-b90f-3ee1a71272b5",
   "metadata": {},
   "source": [
    "Renaming columns in pyspark is also easy.  \n",
    "My question here is can I throw a dictionary of renames and will spark understand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad4631f-66f7-48b3-ac66-4927b69e895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "dfNewFire = dfFire.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(dfNewFire\n",
    "    .select(\"ResponseDelayedinMins\")\n",
    "    .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "    .show(5, False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3419a26-8aed-4d3e-99d4-1c376bf60455",
   "metadata": {},
   "source": [
    "### Mutate columns\n",
    "\n",
    "Modifying the contents of a column or its type are common operations during data\n",
    "exploration. \n",
    "\n",
    "For example, in our SF Fire Department data set, the columns CallDate, WatchDate, and AlarmDtTm are strings rather than either Unix timestamps or SQL dates, both of which Spark supports and can easily manipulate during transformations or actions.\n",
    "\n",
    "`spark.sql.functions` has a set of to/from date/time‐ stamp functions such as `to_timestamp()` and `to_date()` that we can use for just this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8044d1ff-0dd7-4c00-bce4-ff215b4787ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "dfFireTS = (dfNewFire\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "    \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decfea99-aeac-4a7a-bd88-86a19d66bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dfFireTS\n",
    "    .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    "    .show(5, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d5e9f-0bbb-4f8b-854b-ca1f405de556",
   "metadata": {},
   "source": [
    "Unpacked:\n",
    "1. Convert the existing column’s data type from string to a Spark-supported timestamp.\n",
    "2. Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate.\n",
    "3. After converting to the new data type, `drop()` the old column and append the new one specified in the first argument to the `withColumn()` method.\n",
    "4. Assign the new modified DataFrame to `dfFireTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e87fcac-3e52-4569-9f59-08cd2d55fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===============================>                      (116 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "(dfFireTS\n",
    "    .select(year('IncidentDate'))\n",
    "    .distinct()\n",
    "    .orderBy(year('IncidentDate'))\n",
    "    .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965adcb-9d08-464b-b48a-4b7001b2451b",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "What if we want to know what the most common types of fire calls were, or what zip codes accounted for the most calls? These kinds of questions are common in data analysis and exploration.\n",
    "\n",
    "Let’s take our first question: what were the most common types of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a72c741-2055-49c1-8c28-e08c4e8a15ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dfFireTS\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .groupBy(\"CallType\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ee7b9-9d0e-4cc7-8d66-2a4ac2403944",
   "metadata": {},
   "source": [
    "Other common DataFrame operations like `min`, `max` and `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42478d59-800d-4e58-8bcd-f371c253e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|ResponseDelayAverage|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------+--------------------------+--------------------------+\n",
      "|        176170|   3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "(dfFireTS\n",
    ".select(F.sum(\"NumAlarms\"), \n",
    "        F.avg(\"ResponseDelayedinMins\").alias('ResponseDelayAverage'),\n",
    "        F.min(\"ResponseDelayedinMins\"), \n",
    "        F.max(\"ResponseDelayedinMins\"))\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154be65f-616f-42d0-b2ba-e816d6d8310c",
   "metadata": {},
   "source": [
    "## End-to-End DataFrame Example\n",
    "\n",
    "• What were all the different types of fire calls in 2018?   \n",
    "• What months within the year 2018 saw the highest number of fire calls?  \n",
    "• Which neighborhood in San Francisco generated the most fire calls in 2018?  \n",
    "• Which neighborhoods had the worst response times to fire calls in 2018?  \n",
    "• Which week in the year in 2018 had the most fire calls?  \n",
    "• Is there a correlation between neighborhood, zip code, and number of fire calls?   \n",
    "• How can we use Parquet files or SQL tables to store this data and read it back?   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc967ff-af27-447e-a2ec-84a38b9d08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00daab-9865-49eb-b858-165508a7b645",
   "metadata": {},
   "source": [
    "## Typed Objects, Untyped Objects, and Generic Rows\n",
    "\n",
    "In Spark’s supported languages, Datasets make sense only in Java and Scala, whereas in Python and R only DataFrames make sense. This is because Python and R are not compile-time type-safe; types are dynamically inferred or assigned during execution, not during compile time. The reverse is true in Scala and Java.\n",
    "\n",
    "Row is a generic object type in Spark, holding a collection of mixed types that can be\n",
    "accessed using an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1751146b-2d15-4e1a-9943-e35aa9cf52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee39862-234a-437b-a912-164ffea8ec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f26f6199-9e8c-427f-a788-3fa5a7dc5b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55590723-5135-4a4d-a458-c27844450297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Spark 2E'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266824d-8f89-47a4-abd6-24d2dc5b3693",
   "metadata": {},
   "source": [
    "## DataFrames, DataSets and RDDs\n",
    "\n",
    "• If you want to tell Spark what to do, not how to do it, use DataFrames or Datasets.  \n",
    "• If you want rich semantics, high-level abstractions, and DSL operators, use Data‐Frames or Datasets.  \n",
    "• If you want strict compile-time type safety and don’t mind creating multiple case classes for a specific Dataset[T], use Datasets.  \n",
    "• If your processing demands high-level expressions, filters, maps, aggregations, computing averages or sums, SQL queries, columnar access, or use of relational operators on semi-structured data, use DataFrames or Datasets.  \n",
    "• If your processing dictates relational transformations similar to SQL-like queries, use DataFrames.  \n",
    "• If you want to take advantage of and benefit from Tungsten’s efficient serialization with Encoders, use Datasets.  \n",
    "• If you want unification, code optimization, and simplification of APIs across Spark components, use DataFrames.  \n",
    "• If you are an R user, use DataFrames.   \n",
    "• If you are a Python user, use DataFrames and drop down to RDDs if you need more control.  \n",
    "• If you want space and speed efficiency, use DataFrames.  \n",
    "• If you want errors caught during compilation rather than at runtime, choose the appropriate API as depicted in the following figure.   \n",
    "\n",
    "![title](chapter3/img1.png)\n",
    "\n",
    "Use RDDs when\n",
    "\n",
    "• Are using a third-party package that’s written using RDDs  \n",
    "• Can forgo the code optimization, efficient space utilization, and performance benefits available with DataFrames and Datasets   \n",
    "• Want to precisely instruct Spark how to do a query   \n",
    "\n",
    "\n",
    "\n",
    "What’s more, you can seamlessly move between DataFrames or Datasets and RDDs at will using a simple API method call, `df.rdd`. (Note, however, that this does have a cost and should be avoided unless necessary.) After all, DataFrames and Datasets are built on top of RDDs, and they get decomposed to compact RDD code during whole- stage code generation, which we discuss in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b5191-8cce-4e80-9796-78d2d23a83e9",
   "metadata": {},
   "source": [
    "Spark Stack:\n",
    "![title](chapter3/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f946a-0c13-41d0-805f-dbb17d9c476f",
   "metadata": {},
   "source": [
    "### The .explain(True) method\n",
    "\n",
    "![title](chapter3/img3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c03211-22a0-494c-86ff-0ae8a6138921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, RowID#82, ... 4 more fields]\n",
      "+- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, ... 5 more fields]\n",
      "   +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, ... 4 more fields]\n",
      "      +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, ... 5 more fields]\n",
      "         +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, ... 4 more fields]\n",
      "            +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallDate#60, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, ... 5 more fields]\n",
      "               +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallDate#60, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, ... 4 more fields]\n",
      "                  +- Relation[CallNumber#56,UnitID#57,IncidentNumber#58,CallType#59,CallDate#60,WatchDate#61,CallFinalDisposition#62,AvailableDtTm#63,Address#64,City#65,Zipcode#66,Battalion#67,StationArea#68,Box#69,OriginalPriority#70,Priority#71,FinalPriority#72,ALSUnit#73,CallTypeGroup#74,NumAlarms#75,UnitType#76,UnitSequenceInCallDispatch#77,FirePreventionDistrict#78,SupervisorDistrict#79,... 4 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "CallNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallFinalDisposition: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: string, Box: string, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceInCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: string, Neighborhood: string, Location: string, RowID: string, ... 4 more fields\n",
      "Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, RowID#82, ... 4 more fields]\n",
      "+- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, ... 5 more fields]\n",
      "   +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, ... 4 more fields]\n",
      "      +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, ... 5 more fields]\n",
      "         +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, ... 4 more fields]\n",
      "            +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallDate#60, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, ... 5 more fields]\n",
      "               +- Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallDate#60, WatchDate#61, CallFinalDisposition#62, AvailableDtTm#63, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, ... 4 more fields]\n",
      "                  +- Relation[CallNumber#56,UnitID#57,IncidentNumber#58,CallType#59,CallDate#60,WatchDate#61,CallFinalDisposition#62,AvailableDtTm#63,Address#64,City#65,Zipcode#66,Battalion#67,StationArea#68,Box#69,OriginalPriority#70,Priority#71,FinalPriority#72,ALSUnit#73,CallTypeGroup#74,NumAlarms#75,UnitType#76,UnitSequenceInCallDispatch#77,FirePreventionDistrict#78,SupervisorDistrict#79,... 4 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, RowID#82, ... 4 more fields]\n",
      "+- Relation[CallNumber#56,UnitID#57,IncidentNumber#58,CallType#59,CallDate#60,WatchDate#61,CallFinalDisposition#62,AvailableDtTm#63,Address#64,City#65,Zipcode#66,Battalion#67,StationArea#68,Box#69,OriginalPriority#70,Priority#71,FinalPriority#72,ALSUnit#73,CallTypeGroup#74,NumAlarms#75,UnitType#76,UnitSequenceInCallDispatch#77,FirePreventionDistrict#78,SupervisorDistrict#79,... 4 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [CallNumber#56, UnitID#57, IncidentNumber#58, CallType#59, CallFinalDisposition#62, Address#64, City#65, Zipcode#66, Battalion#67, StationArea#68, Box#69, OriginalPriority#70, Priority#71, FinalPriority#72, ALSUnit#73, CallTypeGroup#74, NumAlarms#75, UnitType#76, UnitSequenceInCallDispatch#77, FirePreventionDistrict#78, SupervisorDistrict#79, Neighborhood#80, Location#81, RowID#82, ... 4 more fields]\n",
      "+- FileScan csv [CallNumber#56,UnitID#57,IncidentNumber#58,CallType#59,CallDate#60,WatchDate#61,CallFinalDisposition#62,AvailableDtTm#63,Address#64,City#65,Zipcode#66,Battalion#67,StationArea#68,Box#69,OriginalPriority#70,Priority#71,FinalPriority#72,ALSUnit#73,CallTypeGroup#74,NumAlarms#75,UnitType#76,UnitSequenceInCallDispatch#77,FirePreventionDistrict#78,SupervisorDistrict#79,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/portugapt/Documents/LearnHub/Books/Learning Spark: Lightning-Fast Da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CallNumber:int,UnitID:string,IncidentNumber:int,CallType:string,CallDate:string,WatchDate:...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/08 22:21:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "dfFireTS.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518a9e8-2b47-40f4-94ab-ae581d36711f",
   "metadata": {},
   "source": [
    "## Summary (Cropped)\n",
    "\n",
    "Through illustrative common data operations and code examples, we demonstrated that the high-level DataFrame and Dataset APIs are far more expressive and intuitive than the low-level RDD API. Designed to make processing of large data sets easier, the Structured APIs provide domain-specific operators for common data operations, increasing the clarity and expressiveness of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f49701-081f-4bc9-aa61-a7cc4416ce0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
