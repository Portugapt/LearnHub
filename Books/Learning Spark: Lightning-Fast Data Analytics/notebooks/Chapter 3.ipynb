{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0480ce25-f2a5-4b2e-a039-f3f291da0a3f",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2cf8f-d03e-4f9d-a5f8-5197eca9944c",
   "metadata": {},
   "source": [
    "## Spark: What’s Underneath an RDD?\n",
    "\n",
    "• Dependencies   \n",
    "• Partitions (with some locality information)   \n",
    "• Compute function: Partition => Iterator[T]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664139c-eaf6-41c0-be42-a44247dd4331",
   "metadata": {},
   "source": [
    "Spark 2.x introduced a few key schemes for structuring Spark. One is to express com‐\n",
    "putations by using common patterns found in data analysis. These patterns are\n",
    "expressed as high-level operations such as filtering, selecting, counting, aggregating,\n",
    "averaging, and grouping. This provides added clarity and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6f9ed-5ec8-46ea-ae44-5f9a8f9392c1",
   "metadata": {},
   "source": [
    "This specificity is further narrowed through the use of a set of common operators in a\n",
    "DSL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793cdc9-018e-450f-bf87-702de2dae90a",
   "metadata": {},
   "source": [
    "## Schemas and Creating DataFrames\n",
    "\n",
    "Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "    \n",
    "• You relieve Spark from the onus of inferring data types.  \n",
    "• You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.  \n",
    "• You can detect errors early if data doesn’t match the schema.   \n",
    "\n",
    "Here's an example how to define schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de523405-e41a-4cf0-9844-83ad6a2cf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "#sc.setLogLevel(newLevel)\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "        \"LinkedIn\"]],\n",
    "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "        \"LinkedIn\"]],\n",
    "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "        [\"twitter\", \"FB\"]],\n",
    "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "        [\"twitter\", \"LinkedIn\"]]\n",
    "        ]\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession\n",
    "    spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Example-3_6\")\n",
    "        .getOrCreate())\n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    # Show the DataFrame; it should reflect our table above\n",
    "    blogs_df.show()\n",
    "    # Print the schema used by Spark to process the DataFrame\n",
    "    print(blogs_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7d7cd-46d2-41d4-803b-095cfe44ef96",
   "metadata": {},
   "source": [
    "[Here's a link to the documentation on Python](http://spark.apache.org/docs/latest/api/python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ce429-c6b1-4b3e-88c3-599d7df74494",
   "metadata": {},
   "source": [
    "To start with the basics, lets look at the [columns API](http://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)\n",
    "\n",
    "If run into any dataType issue,\n",
    "```Column.cast(dataType)``` converts the column into type dataType.\n",
    "...\n",
    "Keep exploring.   \n",
    "\n",
    "On Rows, here are examples of how to make quick rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1549ec76-1ce1-43b2-bd04-28b79c748dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2d9e72-3067-4b7c-96a2-7abbfa74ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3efde9-a963-4e4b-b02c-d63bc8edf917",
   "metadata": {},
   "source": [
    "To read from a file: ```DataFrameReader```   \n",
    "To save to a file: ```DataFrameWriter```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a46d23-dbdd-4444-86e8-a4379d46a940",
   "metadata": {},
   "source": [
    "## Hands on a Dataset\n",
    "Quote:\n",
    "To get started, let’s read a large CSV file containing data on San Francisco Fire\n",
    "Department calls.1 As noted previously, we will define a schema for this file and use\n",
    "the DataFrameReader class and its methods to tell Spark what to do. Because this file\n",
    "contains 28 columns and over 4,380,660 records,2 it’s more efficient to define a\n",
    "schema than have Spark infer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdf764-463e-49a0-8a58-6b106ae4bfb5",
   "metadata": {},
   "source": [
    "First lets define the schema for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2e6b0a-5706-4eaa-8e2d-d77471caf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "            StructField('UnitID', StringType(), True),\n",
    "            StructField('IncidentNumber', IntegerType(), True),\n",
    "            StructField('CallType', StringType(), True),\n",
    "            StructField('CallDate', StringType(), True),\n",
    "            StructField('WatchDate', StringType(), True),\n",
    "            StructField('CallFinalDisposition', StringType(), True),\n",
    "            StructField('AvailableDtTm', StringType(), True),\n",
    "            StructField('Address', StringType(), True),\n",
    "            StructField('City', StringType(), True),\n",
    "            StructField('Zipcode', IntegerType(), True),\n",
    "            StructField('Battalion', StringType(), True),\n",
    "            StructField('StationArea', StringType(), True),\n",
    "            StructField('Box', StringType(), True),\n",
    "            StructField('OriginalPriority', StringType(), True),\n",
    "            StructField('Priority', StringType(), True),\n",
    "            StructField('FinalPriority', IntegerType(), True),\n",
    "            StructField('ALSUnit', BooleanType(), True),\n",
    "            StructField('CallTypeGroup', StringType(), True),\n",
    "            StructField('NumAlarms', IntegerType(), True),\n",
    "            StructField('UnitType', StringType(), True),\n",
    "            StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "            StructField('FirePreventionDistrict', StringType(), True),\n",
    "            StructField('SupervisorDistrict', StringType(), True),\n",
    "            StructField('Neighborhood', StringType(), True),\n",
    "            StructField('Location', StringType(), True),\n",
    "            StructField('RowID', StringType(), True),\n",
    "            StructField('Delay', FloatType(), True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcec302-65ea-4b15-9e99-271e55fce634",
   "metadata": {},
   "source": [
    "Now, lets read the dataset file. ```dfFire``` is read from the defined file, and with the schema defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d5687e-3bc3-40ec-8f3c-e6c851ea8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_fire_file = \"../repo/chapter3/data/sf-fire-calls.csv\"\n",
    "dfFire = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4baf107-ec2f-44d8-8f08-469c0e357cd7",
   "metadata": {},
   "source": [
    "To save the dataframe, for example to Parquet, which saves the schema in it's metadata, it's as simple as:   \n",
    "```python\n",
    "parquet_path = ...\n",
    "fire_df.write.format(\"parquet\").save(parquet_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0927fd-bb16-4ecf-b4a4-180eb5553018",
   "metadata": {},
   "source": [
    "Examples of some filters and projections\n",
    "A projection in relational parlance is a way to return only the\n",
    "rows matching a certain relational condition by using filters. In Spark, projections are\n",
    "done with the select() method, while filters can be expressed using the filter() or\n",
    "where() method. We can use this technique to examine specific aspects of our SF Fire\n",
    "Department data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b60dd68-3d4d-403e-bfb6-bf1226092cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfFewFires = (dfFire\n",
    "        .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "        .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "dfFewFires.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a28c05-8f2e-4c89-9638-0fd9122aeb9d",
   "metadata": {},
   "source": [
    "```countDistinct```: Returns a new Column for distinct count of col or cols.   \n",
    "What if we want to know how many distinct CallTypes were recorded as the causes\n",
    "of the fire calls? These simple and expressive queries do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0b4edbe-f307-446f-adb8-0c10147f13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>   (186 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "(dfFire\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    "    .show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14838511-d3cb-439a-acc1-4222f8a9d6be",
   "metadata": {},
   "source": [
    "We can list the distinct call types in the data set using these queries:\n",
    "In Python, filter for only distinct non-null CallTypes from all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e8f515a-26b4-4d54-b377-265a1dcc0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dfFire\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .distinct()\n",
    "    .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ab06f-9c5b-40a2-b90f-3ee1a71272b5",
   "metadata": {},
   "source": [
    "Renaming columns in pyspark is also easy.  \n",
    "My question here is can I throw a dictionary of renames and will spark understand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aad4631f-66f7-48b3-ac66-4927b69e895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "dfNewFIre = dfFire.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(dfNewFIre\n",
    "    .select(\"ResponseDelayedinMins\")\n",
    "    .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "    .show(5, False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ab167-48bc-444a-9bc2-db7f56984c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
