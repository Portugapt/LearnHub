{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82c25b3-7c90-46a5-8616-0b37275b419f",
   "metadata": {},
   "source": [
    "# Spark SQL and DataFrames: Interacting with External Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d5fa3a-1743-4fcb-8272-e8aeb1dc4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/13 18:55:48 WARN Utils: Your hostname, OutOne resolves to a loopback address: 127.0.1.1; using 192.168.1.84 instead (on interface enp9s0)\n",
      "21/08/13 18:55:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/portugapt/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/13 18:55:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Chapter 5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712037d-d29a-45d1-86d9-12098298ba5e",
   "metadata": {},
   "source": [
    "In the previous chapter, we explored interacting with the built-in data sources in Spark. We also took a closer look at the DataFrame API and its interoperability with Spark SQL. In this chapter, we will focus on how Spark SQL interfaces with external components. Specifically, we discuss how Spark SQL allows you to:   \n",
    "* Use user-defined functions for both Apache Hive and Apache Spark.   \n",
    "* Connect with external data sources such as JDBC and SQL databases, PostgreSQL, MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.   \n",
    "* Work with simple and complex types, higher-order functions, and common relational operators.   \n",
    "\n",
    "We’ll also look at some different options for querying Spark using Spark SQL, such as the Spark SQL shell, Beeline, and Tableau.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd543c-11d0-4dec-a844-35fb909f8333",
   "metadata": {},
   "source": [
    "## Spark SQL and Apache Hive\n",
    "\n",
    "Spark SQL is a foundational component of Apache Spark that integrates relational processing with Spark’s functional programming API. Its genesis was in previous work on Shark. Shark was originally built on the Hive codebase on top of Apache Spark and became one of the first interactive SQL query engines on Hadoop systems.  \n",
    "It demonstrated that it was possible to have the best of both worlds; as fast as an enterprise data warehouse, and scaling as well as Hive/MapReduce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c861e-b9f6-4248-a81a-321e2f82d661",
   "metadata": {},
   "source": [
    "## User-defined functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee24d9d-e2c5-4996-b73a-8ff49c31350d",
   "metadata": {},
   "source": [
    "The benefit of creating your own PySpark or Scala UDFs is that you (and others) will be able to make use of them within Spark SQL itself.  \n",
    "For example, a data scientist can wrap an ML model within a UDF so that a data analyst can query its predictions in Spark SQL **without necessarily understanding the internals of the model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a0057a-f4aa-48df-b0c8-4b75e0a20812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d138768-3dd9-4fb0-bede-28a40e0fe7a0",
   "metadata": {},
   "source": [
    "And now I can use this function by simply calling it on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20836db3-dd03-4ddd-b1ce-b4a82ed3cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215736a0-a54b-4c68-af35-18d5959ad906",
   "metadata": {},
   "source": [
    "## Speeding up and distributing PySpark UDFs with Pandas UDFs\n",
    "\n",
    "With Apache Spark 3.0, Pandas UDFs infer the Pandas UDF type from Pythontype hints in Pandas UDFs such as pandas.Series, pandas.DataFrame, Tuple, and Iterator. Previously you needed to manually define and specify each Pandas UDF type. Currently, the supported cases of Python type hints in Pandas UDFs are Series to Series, Iterator of Series to Iterator of Series, Iterator of Multiple Series to Iterator of Series, and Series to Scalar (a single value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a52b6-99a8-4278-ab62-51984d534a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "# Create the pandas UDF for the cubed function\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1d3c62-10f5-4588-82d4-6074c4a64699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(a: pandas.core.series.Series) -> pandas.core.series.Series>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubed_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d8d272-2c64-4d35-aefe-8ad8abe6a521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 µs ± 8.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Create a Pandas Series\n",
    "x = pd.Series([1, 2, 3])\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "cubed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d511e6f-bf8d-44b1-8ce1-8b79e7b957f3",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc925be3-8890-471e-b460-5ec1d34f7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import DataFrame\n",
    "#u.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932c1c8-de2c-424a-a55d-3823163465cb",
   "metadata": {},
   "source": [
    "## PostgreSQL\n",
    "To connect to a PostgreSQL database, build or download the JDBC jar from [Maven](https://mvnrepository.com/artifact/org.postgresql/postgresql) and add it to your classpath. Then start a Spark shell (spark-shell or pyspark), specifying that jar:    \n",
    "`bin/spark-shell --jars postgresql-42.2.6.jar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a82054-9e1e-4e8e-83aa-c3b3cd283846",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:108)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:108)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44429/3031051409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read Option 1: Loading data from a JDBC source using load method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m jdbcDF1 = (spark\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jdbc:postgresql://[DBSERVER]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:108)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:108)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Read Option 1: Loading data from a JDBC source using load method\n",
    "\"\"\"\n",
    "jdbcDF1 = (spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .load()\n",
    "            )\n",
    "\n",
    "# Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "jdbcDF2 = (spark\n",
    "    .read\n",
    "    .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "    properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"})\n",
    "          )\n",
    "# Write Option 1: Saving data to a JDBC source using save method\n",
    "(jdbcDF1\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save()\n",
    ")\n",
    "# Write Option 2: Saving data to a JDBC source using jdbc method\n",
    "(jdbcDF2\n",
    "    .write\n",
    "    .jdbc(\"jdbc:postgresql:[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "    properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"})\n",
    ")\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905ed02-6cb0-431d-905c-d815d2f046d6",
   "metadata": {},
   "source": [
    "## Higher-Order Functions\n",
    "\n",
    "[PySpark Built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "Higher-order functions that take anonymous lambda functions as arguments. An example of a higher-\n",
    "order function is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e144115-c431-484c-85b0-433b8d105f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "# Show the DataFrame\n",
    "t_c.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d22b71-75ab-4a99-bdf8-4694bb0077db",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "`transform(array<T>, function<T, U>): array<U>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cbe9af-05a6-41ad-8a20-bc6b8fd94973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372b6a4b-d855-4785-92e9-4effaf36bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.15 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef05567-df1e-450e-b5d7-f514d257204b",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "`filter(array<T>, function<T, Boolean>): array<T>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb89a1b-fa00-4ae2-b676-fcea143e88b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "    SELECT celsius,\n",
    "    filter(celsius, t -> t > 38) as high\n",
    "    FROM tC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f809108-e579-4c47-816c-38da3faa10e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.43 ms ± 450 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "spark.sql(\"\"\"\n",
    "    SELECT celsius,\n",
    "    filter(celsius, t -> t > 38) as high\n",
    "    FROM tC\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96716b5-8bdf-4ad3-92b0-9d7bebd8024c",
   "metadata": {},
   "source": [
    "### Exists\n",
    "\n",
    "`exists(array<T>, function<T, V, Boolean>): Boolean`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4e3b51-8c4e-4a85-a2c8-6b6789cf3b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c32c4a-2aea-4485-99db-faeaa0c08e10",
   "metadata": {},
   "source": [
    "### Aggregate\n",
    "`aggregate(array<T>, B, function<B, T, B>, function<B, R>)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9b8228-29cf-44f1-8be4-d575f899736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT celsius,\n",
    "    aggregate(\n",
    "    celsius,\n",
    "    0,\n",
    "    (t, acc) -> t + acc,\n",
    "    acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "    ) as avgFahrenheit\n",
    "    FROM tC\n",
    "    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13de9bfd-92a0-44ba-8702-ce9220e1b3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.64 ms ± 749 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "spark.sql(\"\"\"\n",
    "    SELECT celsius,\n",
    "    aggregate(\n",
    "    celsius,\n",
    "    0,\n",
    "    (t, acc) -> t + acc,\n",
    "    acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "    ) as avgFahrenheit\n",
    "    FROM tC\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74f94d-f8fb-4e61-9186-ef3f02e34351",
   "metadata": {},
   "source": [
    "### Unions, Joins, etc\n",
    "\n",
    "Perform these DataFrame operations, we’ll first prepare some data. In the following code snippet, we:   \n",
    "1. Import two files and create two DataFrames, one for airport (airportsna) information and one for US flight delays (departureDelays).\n",
    "2. Using expr(), convert the delay and distance columns from STRING to INT.\n",
    "3. Create a smaller table, foo, that we can focus on for our demo examples; it contains only information on three flights originating from Seattle (SEA) to the destination of San Francisco (SFO) for a small time range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8ad2f0e-1fa9-466d-802a-884d2713e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "from pyspark.sql.functions import expr\n",
    "tripdelaysFilePath =\"../repo/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airportsnaFilePath =\"../repo/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "    .load(airportsnaFilePath))\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "    \n",
    "\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .options(header=\"true\")\n",
    "    .load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    "    .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "    .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "\n",
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    "    .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
    "    date like '01010%' and delay > 0\"\"\")))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49caaae6-894a-44b3-8c72-f12d33e5fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "273ee828-acac-4a65-acb8-91dfe568cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22c6b348-8eac-4081-83eb-da330e8635cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e9de87e-d9be-4b07-824d-0350806fd7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f3f3a89-eee5-4409-8f85-44613d1855d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(\n",
    "    airportsna,\n",
    "    airportsna.IATA == foo.origin\n",
    "    ).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18f2b39b-4ab2-4a48-94aa-5b7b820bfdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "    FROM foo f\n",
    "    JOIN airports_na a\n",
    "    ON a.IATA = f.origin\n",
    "    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ef8b0-ab06-4da2-b146-adb49939020e",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "A window function uses values from the rows in a window (a range of input rows) to return a set of values, typically in the form of another row. With window functions, it is possible to operate on a group of rows while still returning a single value for every input row.\n",
    "\n",
    "Let’s start with a review of the TotalDelays (calculated by sum(Delay)) experienced by flights originating from Seattle (SEA), San Francisco (SFO), and New York City (JFK) and going to a specific set of destination locations.\n",
    "\n",
    "What if for each of these origin airports you wanted to find the three destinations that\n",
    "experienced the most delays? You could achieve this by running three different quer‐\n",
    "ies for each origin and then unioning the results together, like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a5a65a7-0cbe-4484-bde7-bb1798ce4eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "+------+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfView = spark.sql(\"\"\"SELECT\n",
    "     origin, destination, SUM(delay) AS TotalDelays\n",
    "    FROM\n",
    "     departureDelays\n",
    "    WHERE\n",
    "     origin IN ('SEA', 'SFO', 'JFK')\n",
    "    AND\n",
    "     destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "    GROUP\n",
    "     BY origin, destination;\n",
    "    \"\"\")\n",
    "\n",
    "dfView.createOrReplaceTempView(\"departureDelaysWindow\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT origin, destination, TotalDelays, rank\n",
    "    FROM (\n",
    "    SELECT origin, destination, TotalDelays, dense_rank()\n",
    "    OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "    FROM departureDelaysWindow\n",
    "    ) t\n",
    "    WHERE rank <= 3\n",
    "    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046400b-9578-453b-81b8-e59aa2f4dfa7",
   "metadata": {},
   "source": [
    "By using the dense_rank() window function, we can quickly ascertain which destinations with the worst delays for the three origin cities were."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2067dd6-28d0-4dd5-9bb6-a9f8a89c94d5",
   "metadata": {},
   "source": [
    "### Modifications\n",
    "\n",
    "**Underlying RDDs are immutable**    \n",
    "While DataFrames themselves are immutable, you can modify them through operations that create new, different DataFrames, with different columns, for example. \n",
    "\n",
    "#### Add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8974b90d-fc54-403b-8031-a8c04d9eec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    "    \"status\",\n",
    "    expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76d84836-3a7c-47c6-886c-bf6c7c0a67b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f392b-01d3-4472-9474-ab9942d367a2",
   "metadata": {},
   "source": [
    "#### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d77628f-d8d6-4688-84ee-dcc0bc7566bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7750f3-6cbd-4407-be39-7e3f08e0c009",
   "metadata": {},
   "source": [
    "#### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a3a661e-6dad-412e-9019-96d4a535ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bad32-2bc4-47b7-9bdd-2b835663376c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pivoting\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/\n",
    "\n",
    "```sql\n",
    "\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
