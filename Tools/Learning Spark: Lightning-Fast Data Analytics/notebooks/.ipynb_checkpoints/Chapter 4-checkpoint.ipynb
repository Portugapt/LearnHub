{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94f320d",
   "metadata": {},
   "source": [
    "# Spark SQL and DataFrames: Introduction to Built-in Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0efb6",
   "metadata": {},
   "source": [
    "## Notes from previous chapters\n",
    "\n",
    "_Unexplored_ Spark SQL:\n",
    "* Lets you query data using JDBC/ODBC connectors from external business intel‐ ligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs such as MySQL and PostgreSQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc54e0a",
   "metadata": {},
   "source": [
    "## Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31182078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to data set\n",
    "csv_file = \"../repo/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "dfDelays = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "dfDelays.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d00ae",
   "metadata": {},
   "source": [
    "createOrReplaceTempView Method:\n",
    "\n",
    "Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
    "The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
    "that was used to create this :class:`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2426c459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDelays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2074766",
   "metadata": {},
   "source": [
    "Query the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b46b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2776e",
   "metadata": {},
   "source": [
    "As the results show, all of the longest flights were between Honolulu (HNL) and New\n",
    "York (JFK).  \n",
    "Next, we’ll find all flights between San Francisco (SFO) and Chicago\n",
    "(ORD) with at least a two-hour delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b299e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbc6cb",
   "metadata": {},
   "source": [
    "With CASE Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d761a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "        CASE\n",
    "        WHEN delay > 360 THEN 'Very Long Delays'\n",
    "        WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "        WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "        WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "        WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "        END AS Flight_Delays\n",
    "        FROM us_delay_flights_tbl\n",
    "        ORDER BY origin, delay DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb141d1a",
   "metadata": {},
   "source": [
    "All three of the preceding SQL queries can be expressed with an equivalent Data‐\n",
    "Frame API query.  \n",
    "For example, the first query can be expressed in the Python Data‐\n",
    "Frame API as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf8bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "(dfDelays.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"))).show(10)\n",
    "# Or\n",
    "# (dfDelays.select(\"distance\", \"origin\", \"destination\")\n",
    "#     .where(\"distance > 1000\")\n",
    "#     .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3adfb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "# FROM us_delay_flights_tbl\n",
    "# WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "# ORDER by delay DESC\"\"\").show(10)\n",
    "(dfDelays.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where((col(\"delay\") > 120) & \n",
    "            (col(\"origin\") == 'SFO') & \n",
    "            (col('destination') == 'ORD'))\n",
    "    .orderBy(desc(\"delay\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5da7977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/\n",
    "from pyspark.sql.functions import col, desc, when, asc\n",
    "(dfDelays.select(\"delay\", \"origin\", \"destination\")\n",
    "    .withColumn('Flight_Delays', when((col('delay') > 360), 'Very Long Delays')\n",
    "                                .when((col('delay') > 120) & (col('delay') <= 360), 'Long Delays')\n",
    "                                .when((col('delay') > 60) & (col('delay') <= 120), 'Long Delays')\n",
    "                                .when((col('delay') > 0) & (col('delay') <= 60), 'Long Delays')\n",
    "                                .when(col('delay') == 0, 'No Delays')\n",
    "                                .otherwise('Early'))\n",
    "    .orderBy(asc(\"origin\"), desc(\"delay\")).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67251a26-2148-4ad0-ba00-005afe9159e9",
   "metadata": {},
   "source": [
    "## Creating a managed table\n",
    "\n",
    "To create a managed table within the database learn_spark_db, you can issue a SQL query like the following:\n",
    "\n",
    "Can't get Hive to work.    \n",
    "https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error    \n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html    \n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9a91040-5eff-4cff-87e7-49e255aac35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/12 19:15:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65275/198060928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792b16c-3d7c-4fa6-b474-da6c5b09f8cf",
   "metadata": {},
   "source": [
    "### Temporary views versus global temporary views\n",
    "\n",
    "The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark.   \n",
    "A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application.  \n",
    "Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06156542-1052-4dc7-8aa3-7d4efd1e3ff7",
   "metadata": {},
   "source": [
    "## Reading Tables into DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d31adee2-4fc5-4e7d-aa5c-b893704a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b89b767-f437-44c8-ad9e-9d81e608d3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_flights_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848b7b5-cf50-4344-aea5-f6566d1ba1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b1f25c39e3fd3a9f34544221da3c3ef74531b1cea22214d5f3b3369e52ce34d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
