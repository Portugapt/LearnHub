{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spark SQL and DataFrames: Introduction to Built-in Data Sources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notes from previous chapters\n",
    "\n",
    "_Unexplored_ Spark SQL:\n",
    "* Lets you query data using JDBC/ODBC connectors from external business intel‐ ligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs such as MySQL and PostgreSQL.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Views"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"SparkSQLExampleApp\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = \"../repo/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "dfDelays = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "dfDelays.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/12 00:08:31 WARN Utils: Your hostname, OutOne resolves to a loopback address: 127.0.1.1; using 192.168.1.84 instead (on interface enp9s0)\n",
      "21/08/12 00:08:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/portugapt/.pyenv/versions/3.8.1/envs/LearnHub/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/12 00:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/12 00:08:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "createOrReplaceTempView Method:\n",
    "\n",
    "Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
    "The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
    "that was used to create this :class:`DataFrame`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dfDelays"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query the table "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the results show, all of the longest flights were between Honolulu (HNL) and New\n",
    "York (JFK).  \n",
    "Next, we’ll find all flights between San Francisco (SFO) and Chicago\n",
    "(ORD) with at least a two-hour delay:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "ORDER by delay DESC\"\"\").show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With CASE Clause"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "        CASE\n",
    "        WHEN delay > 360 THEN 'Very Long Delays'\n",
    "        WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "        WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "        WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "        WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "        END AS Flight_Delays\n",
    "        FROM us_delay_flights_tbl\n",
    "        ORDER BY origin, delay DESC\"\"\").show(10)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All three of the preceding SQL queries can be expressed with an equivalent Data‐\n",
    "Frame API query.  \n",
    "For example, the first query can be expressed in the Python Data‐\n",
    "Frame API as:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "(dfDelays.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"))).show(10)\n",
    "# Or\n",
    "# (dfDelays.select(\"distance\", \"origin\", \"destination\")\n",
    "#     .where(\"distance > 1000\")\n",
    "#     .orderBy(\"distance\", ascending=False).show(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "# FROM us_delay_flights_tbl\n",
    "# WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "# ORDER by delay DESC\"\"\").show(10)\n",
    "(dfDelays.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where((col(\"delay\") > 120) & \n",
    "            (col(\"origin\") == 'SFO') & \n",
    "            (col('destination') == 'ORD'))\n",
    "    .orderBy(desc(\"delay\"))).show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/\n",
    "from pyspark.sql.functions import col, desc, when, asc\n",
    "(dfDelays.select(\"delay\", \"origin\", \"destination\")\n",
    "    .withColumn('Flight_Delays', when((col('delay') > 360), 'Very Long Delays')\n",
    "                                .when((col('delay') > 120) & (col('delay') <= 360), 'Long Delays')\n",
    "                                .when((col('delay') > 60) & (col('delay') <= 120), 'Long Delays')\n",
    "                                .when((col('delay') > 0) & (col('delay') <= 60), 'Long Delays')\n",
    "                                .when(col('delay') == 0, 'No Delays')\n",
    "                                .otherwise('Early'))\n",
    "    .orderBy(asc(\"origin\"), desc(\"delay\")).show(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('LearnHub': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "interpreter": {
   "hash": "9b1f25c39e3fd3a9f34544221da3c3ef74531b1cea22214d5f3b3369e52ce34d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}