{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5ba9e4-dcbc-42c1-bd4f-16ac294ed3fd",
   "metadata": {},
   "source": [
    "# Regression Models with Multiple Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295042e5-070f-4bd1-924a-d249bfc95e54",
   "metadata": {},
   "source": [
    "## Omitted Variable Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779303c-44c0-408b-8a32-0ef738f82786",
   "metadata": {},
   "source": [
    "The previous analysis of the relationship between test score and class size discussed in Chapters 4 and 5 has a major flaw: we ignored other determinants of the dependent variable (test score) that correlate with the regressor (class size). Remember that influences on the dependent variable which are not captured by the model are collected in the error term, which we so far assumed to be uncorrelated with the regressor. However, this assumption is violated if we exclude determinants of the dependent variable which vary with the regressor. This might induce an estimation bias, i.e., the mean of the OLS estimator’s sampling distribution is no longer equals the true mean. In our example we therefore wrongly estimate the causal effect on test scores of a unit change in the student-teacher ratio, on average. This issue is called omitted variable bias (OVB) and is summarized by Key Concept 6.1.\n",
    "\n",
    "![title](images/chapter6/img1.jpg)\n",
    "\n",
    "![title](images/chapter6/img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6a517-fc3b-453b-884a-a53fde1945f3",
   "metadata": {},
   "source": [
    "## Multiple regression model\n",
    "\n",
    "![title](images/chapter6/img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130a69d-ec17-4c12-a1c7-266f9baa20e7",
   "metadata": {},
   "source": [
    "We want to minimize $$ \\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - b_2 X_{2i} - \\dots -  b_k X_{ki})^2 \\tag{6.5} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7595f-bb05-4ff4-91f2-ea1bdd79e570",
   "metadata": {},
   "source": [
    "SER <- sqrt(1/(n-k-1) * SSR)                    # standard error of the regression   \n",
    "Rsq <- 1 - (SSR / TSS)                          # R^2   \n",
    "adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS          # adj. R^2   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c10ee-c52e-49f0-a489-65caa12aff78",
   "metadata": {},
   "source": [
    "As already mentioned, $\\bar{R}^2$ may be used to quantify how good a model fits the data. However, it is rarely a good idea to maximize these measures by stuffing the model with regressors. You will not find any serious study that does so. Instead, it is more useful to include regressors that improve the estimation of the causal effect of interest which is not assessed by means the $\\bar{R}^2$ of the model. The issue of variable selection is covered in Chapter 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abe9fa-36ea-40fb-a170-0c43f1fc58d7",
   "metadata": {},
   "source": [
    "## Imperfect Multicollinearity\n",
    "\n",
    "If X1 and X2 are highly correlated, OLS struggles to precisely estimate β1.   \n",
    "That means that although ^β1 is a consistent and unbiased estimator for β1, it has a large variance due to X2 being included in the model.   \n",
    "\n",
    "https://www.econometrics-with-r.org/6-4-ols-assumptions-in-multiple-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d308169-8cfd-4cb6-bc10-cc74ea7010a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
